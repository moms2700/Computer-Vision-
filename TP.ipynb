{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook SLTP (Setup, Load, Train, Predict)\n",
    "\n",
    "Ce notebook illustre un flux d'exemple pour la formation rapide d'un modèle de langage en 4 étapes :\n",
    "\n",
    "1. **S (Setup)** : Installation et import des librairies.\n",
    "2. **L (Load)** : Chargement d'un dataset et d'un modèle pré-entraîné.\n",
    "3. **T (Train)** : Fine-tuning du modèle sur un mini-exemple.\n",
    "4. **P (Predict)** : Génération de texte avec le modèle fine-tuné.\n",
    "\n",
    "L'objectif est d'être simple et clair pour démarrer rapidement avec la librairie [Hugging Face Transformers](https://github.com/huggingface/transformers).\n",
    "\n",
    "---\n",
    "## Avertissements\n",
    "- Les paramètres d'entraînement sont volontairement petits pour éviter un coût en temps/mémoire trop élevé.\n",
    "- Dans un vrai projet, vous devez utiliser un plus grand dataset et ajuster ces paramètres.\n",
    "- Il est conseillé d'utiliser un **GPU** (local ou cloud) pour accélérer l'entraînement, surtout si vous augmentez la taille des données.\n",
    "\n",
    "Allons-y !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. S - Setup\n",
    "\n",
    "Dans cette première étape, nous allons :\n",
    "1. Vérifier la version de Python.\n",
    "2. Installer si besoin les dépendances (Transformers, Datasets, Torch).\n",
    "3. Importer les modules nécessaires.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(f\"Version de Python: {sys.version}\")\n",
    "\n",
    "# Si besoin, décommentez pour installer les librairies :\n",
    "# %pip install --upgrade pip\n",
    "# %pip install transformers datasets torch\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Librairies importées avec succès !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. L - Load\n",
    "\n",
    "Dans cette seconde étape, nous allons :\n",
    "1. Charger un dataset d’exemple (ici, [Yelp Polarity](https://huggingface.co/datasets/yelp_polarity)).\n",
    "2. Sélectionner un modèle déjà pré-entraîné (ex: `distilgpt2`).\n",
    "3. Préparer le tokenizer et le modèle.\n",
    "\n",
    "Pour réduire la durée de l'exemple, on n'utilise qu'une petite portion du dataset (1%).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1) Chargement du dataset (1% pour la démo)\n",
    "dataset = load_dataset(\"yelp_polarity\", split=\"train[:1%]\")\n",
    "\n",
    "# 2) Choix du modèle (GPT-2 \"mini\" = distilgpt2)\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "print(\"Dataset et modèle chargés !\")\n",
    "print(f\"Exemple d'élément du dataset: {dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. T - Train\n",
    "\n",
    "Dans cette étape, nous allons :\n",
    "1. Tokenizer nos données.\n",
    "2. Préparer un **Trainer** de la librairie Transformers.\n",
    "3. Lancer l'entraînement (fine-tuning).\n",
    "\n",
    "Nous utilisons un batch_size très petit et **1** époque pour accélérer la démonstration.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1) Tokenisation\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# On supprime les colonnes inutiles\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\", \"label\"])\n",
    "\n",
    "# 2) Paramètres d'entraînement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"distilgpt2-sltp-model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=50,\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "# 3) Création du Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    ")\n",
    "\n",
    "print(\"Début de l'entraînement...\")\n",
    "trainer.train()\n",
    "print(\"Entraînement terminé !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. P - Predict (Génération)\n",
    "\n",
    "Maintenant que le modèle est entraîné, nous allons :\n",
    "1. Passer le modèle en mode évaluation.\n",
    "2. Lui donner un prompt d’exemple.\n",
    "3. Générer un texte et l’afficher.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Passage en mode évaluation\n",
    "model.eval()\n",
    "\n",
    "# Prompt de test\n",
    "prompt = \"This restaurant is amazing because\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Génération\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=30,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "# Décodage du texte généré\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"----- Prompt -----\")\n",
    "print(prompt)\n",
    "print(\"\\n----- Generated Text -----\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Nous avons complété le flux **S-L-T-P** :\n",
    "- **Setup** : importation et installation des librairies.\n",
    "- **Load** : chargement du dataset et du modèle.\n",
    "- **Train** : fine-tuning rapide.\n",
    "- **Predict** : génération d'un texte avec le modèle.\n",
    "\n",
    "Pour aller plus loin :\n",
    "- Utiliser un GPU, un dataset plus large et augmenter le nombre d’époques.\n",
    "- Essayer d’autres modèles (GPT-2 complet, Bloom, Falcon, etc.).\n",
    "- Ajuster la **max_length**, le **batch_size**, le **learning_rate**, etc.\n",
    "- Explorer la [documentation Hugging Face](https://huggingface.co/docs) pour plus d’options (ex: DistilBERT pour la classification).\n",
    "\n",
    "Bonne exploration !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "name": "SLTP Notebook"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
